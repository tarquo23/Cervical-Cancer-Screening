{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3UuNR-ZuekKp",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import cv2\n",
        "import csv\n",
        "import os \n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import six\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "\n",
        "np.random.seed(2016)\n",
        "random.seed(2016)\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input, Activation, merge, Dense, Flatten, concatenate, GlobalAveragePooling2D, Dropout\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from numpy import expand_dims\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PraBn_rhxWNF",
        "colab_type": "text"
      },
      "source": [
        "**Mount drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhtIZ0xImxtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf-721isUfbF",
        "colab_type": "text"
      },
      "source": [
        "**Import external files containing utilities funcitons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK0Cp6-bxYtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# external notebooks are located in /content/drive/My Drive/Colab Notebooks/\n",
        "\n",
        "%run '/content/drive/My Drive/Colab Notebooks/dataset_utilities.ipynb'\n",
        "%run '/content/drive/My Drive/Colab Notebooks/resnet_builder.ipynb'\n",
        "%run '/content/drive/My Drive/Colab Notebooks/general_utilities.ipynb'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jkG0YHpek5m",
        "colab_type": "text"
      },
      "source": [
        "**Removes autoscroll throughout process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjA3lY4sfDTG",
        "colab_type": "code",
        "outputId": "072f44ca-4f6d-4fd8-a0a8-c9026c18a564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%%javascript\n",
        "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
        "    return false;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
              "    return false;\n",
              "}"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXBhOXolfq-0",
        "colab_type": "text"
      },
      "source": [
        "**Global declarations**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-woEoxRlftwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf = dict()\n",
        "\n",
        "# How many patients will be in train and validation set during training. Range: (0; 1)\n",
        "conf['train_valid_fraction'] = 0.75\n",
        "\n",
        "# Batch size for CNN [Depends on GPU and memory available]\n",
        "conf['batch_size'] = 64\n",
        "\n",
        "# Number of epochs for CNN training\n",
        "#conf['nb_epoch'] = 200\n",
        "conf['nb_epoch'] = 30\n",
        "\n",
        "# Early stopping. Stop training after epochs without improving on validation\n",
        "conf['earlystopping_patience'] = 90\n",
        "\n",
        "# Shape of image for CNN (Larger the better, but you need to increase CNN as well)\n",
        "#conf['image_shape'] = (4160,4128)\n",
        "#conf['image_shape'] = (2080,2064)\n",
        "#conf['image_shape'] = (1024,1024)\n",
        "conf['image_shape'] = (224,224)\n",
        "#conf['image_shape'] = (64,64)\n",
        "\n",
        "#conf['optimizer'] = ('adam',dict())\n",
        "#conf['optimizer'][1]['lr'] = 0.001\n",
        "conf['optimizer'] = ('sgd',dict())\n",
        "conf['optimizer'][1]['lr'] = 0.01\n",
        "#conf['optimizer'] = ('adadelta',dict())\n",
        "#conf['optimizer'][1]['lr'] = 1.0\n",
        "\n",
        "conf['load_in_ram'] = True\n",
        "\n",
        "# only one of these two fields below can be True\n",
        "conf['blurred_images'] = False\n",
        "conf['cropped_images'] = True\n",
        "\n",
        "conf['use_additional_images'] = True\n",
        "\n",
        "conf['transfer_learning'] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvB690sdfu5e",
        "colab_type": "text"
      },
      "source": [
        "**Hardcoded paths to training files. Note that the \"additional\" directories have been left out**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSlN5FjxfvCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file paths to training \n",
        "if conf['blurred_images'] == False and conf['cropped_images'] == False:\n",
        "  extract_zip_dataset()\n",
        "\n",
        "  filepaths = []  \n",
        "  filepaths.append('/content/train/train/Type_1/')\n",
        "  filepaths.append('/content/train/train/Type_2/')\n",
        "  filepaths.append('/content/train/train/Type_3/')\n",
        "elif conf['blurred_images'] == True: \n",
        "  filepaths = []  \n",
        "  filepaths.append('/content/drive/My Drive/blurred_dataset/Type_1/')\n",
        "  filepaths.append('/content/drive/My Drive/blurred_dataset/Type_2/')\n",
        "  filepaths.append('/content/drive/My Drive/blurred_dataset/Type_3/')\n",
        "elif conf['cropped_images'] == True:\n",
        "  filepaths = []  \n",
        "  filepaths.append('/content/drive/My Drive/original_dataset_manualcropped/Type_1/')\n",
        "  filepaths.append('/content/drive/My Drive/original_dataset_manualcropped/Type_2/')\n",
        "  filepaths.append('/content/drive/My Drive/original_dataset_manualcropped/Type_3/')\n",
        "\n",
        "  if conf['use_additional_images'] == True:\n",
        "    filepaths.append('/content/drive/My Drive/full_additional_dataset_automatedcropped/Type_1/')\n",
        "    filepaths.append('/content/drive/My Drive/full_additional_dataset_automatedcropped/Type_2/')\n",
        "    filepaths.append('/content/drive/My Drive/full_additional_dataset_automatedcropped/Type_3/')\n",
        "\n",
        "else:\n",
        "  raise Exception('Error: no dataset configuration found')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J75J4HmfvKX",
        "colab_type": "text"
      },
      "source": [
        "**Get a list of all training files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewwXp80yfvei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allFiles = []\n",
        "\n",
        "for i, filepath in enumerate(filepaths):\n",
        "    files = glob.glob(filepath + '*.jpg')\n",
        "    allFiles = allFiles + files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saTFFAQ4hM6c",
        "colab_type": "text"
      },
      "source": [
        "**Split data into training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnCcRc3mhQDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if conf['cropped_images'] == False:\n",
        "  split_point = int(round(conf['train_valid_fraction']*len(allFiles)))\n",
        "\n",
        "  random.shuffle(allFiles)\n",
        "\n",
        "  train_list = allFiles[:split_point]\n",
        "  valid_list = allFiles[split_point:]\n",
        "  print('Train patients: {}'.format(len(train_list)))\n",
        "  print('Valid patients: {}'.format(len(valid_list)))\n",
        "  \n",
        "elif conf['use_additional_images'] == False:\n",
        "  train_list = allFiles[:]\n",
        "  print('Train patients: {}'.format(len(train_list)))\n",
        "\n",
        "  filepaths_validation = []\n",
        "  filepaths_validation.append('/content/drive/My Drive/cropped_additional_dataset/Type_1/')\n",
        "  filepaths_validation.append('/content/drive/My Drive/cropped_additional_dataset/Type_2/')\n",
        "  filepaths_validation.append('/content/drive/My Drive/cropped_additional_dataset/Type_3/')\n",
        "\n",
        "  allFiles_validation = []\n",
        "\n",
        "  for i, filepath in enumerate(filepaths_validation):\n",
        "      files = glob.glob(filepath + '*.jpg')\n",
        "      allFiles_validation = allFiles_validation + files\n",
        "\n",
        "  valid_list = allFiles_validation[:]\n",
        "  print('Valid patients: {}'.format(len(valid_list)))\n",
        "\n",
        "else:\n",
        "  split_point = int(round(conf['train_valid_fraction']*len(allFiles)))\n",
        "\n",
        "  random.shuffle(allFiles)\n",
        "\n",
        "  train_list = allFiles[:split_point]\n",
        "  valid_list = allFiles[split_point:]\n",
        "  print('Train patients: {}'.format(len(train_list)))\n",
        "  print('Valid patients: {}'.format(len(valid_list)))\n",
        "  \n",
        "print('Train batches: {}'.format(np.floor(len(train_list)/conf['batch_size'])))\n",
        "print('Valid batches: {}'.format(np.floor(len(valid_list)/conf['batch_size'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3HlcbcAUQIa",
        "colab_type": "text"
      },
      "source": [
        "**Prepare object for data augmentation: in case of loading samples from disk, move validation dataset in a different folder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl6-eKwqxIOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move validation samples in a different directory to allow\n",
        "# the imagedatagenerator to simply retrieves samples from\n",
        "# train directory. (see the next cell) \n",
        "\n",
        "if conf['load_in_ram'] == False:\n",
        "  !mkdir valid\n",
        "  !mkdir valid/Type_1\n",
        "  !mkdir valid/Type_2\n",
        "  !mkdir valid/Type_3\n",
        "\n",
        "  import shutil\n",
        "\n",
        "  for full_filename in valid_list:\n",
        "    typeN_folder = full_filename[21:27]\n",
        "    filename = full_filename[28:]\n",
        "    destination = 'valid/' + typeN_folder + '/' + filename\n",
        "    shutil.move(full_filename, destination)     # move file from full_filename to destination"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVhIwCuGdxRY",
        "colab_type": "text"
      },
      "source": [
        "**Define data augmentation operations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w2hLveJUQhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# data augmentation operations to be applied on trai dataset\n",
        "augmentator_train = ImageDataGenerator(\n",
        "\t\trotation_range=90,\n",
        "\t\thorizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        ")\n",
        "\n",
        "augmentator_valid = ImageDataGenerator()  # no augmentation for validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJSFcpnKd-oG",
        "colab_type": "text"
      },
      "source": [
        "**If samples loaded from disk, define loader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vspwpGMid_CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "\n",
        "if conf['load_in_ram'] == False:\n",
        "  aug_generator_train = augmentator_train.flow_from_directory(\n",
        "      directory=r\"./train/train/\",\n",
        "      target_size=conf['image_shape'],\n",
        "      color_mode=\"rgb\",\n",
        "      batch_size=conf['batch_size'],\n",
        "      class_mode=\"categorical\",\n",
        "      shuffle=True,\n",
        "  )\n",
        "\n",
        "  aug_generator_valid = augmentator_valid.flow_from_directory(\n",
        "      directory=r\"./valid/\",\n",
        "      target_size=conf['image_shape'],\n",
        "      color_mode=\"rgb\",\n",
        "      batch_size=conf['batch_size'],\n",
        "      class_mode=\"categorical\",\n",
        "      shuffle=False,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  # these two variables will be passed to model.fit_generator\n",
        "  train_it = aug_generator_train\n",
        "  val_it = aug_generator_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPrS-6TVq1lk",
        "colab_type": "text"
      },
      "source": [
        "**Otherwise, samples loaded in ram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIPtglAvq15P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if conf['load_in_ram'] == True:\n",
        "  train_imgs_in_ram, train_labels_in_ram = load_dataset_in_ram(train_list)\n",
        "  val_imgs_in_ram, val_labels_in_ram = load_dataset_in_ram(valid_list)\n",
        "\n",
        "  print('train_imgs_in_ram: {}'.format(len(train_imgs_in_ram)))\n",
        "  print('train_labels_in_ram: {}'.format(len(train_labels_in_ram)))\n",
        "  print('val_imgs_in_ram: {}'.format(len(val_imgs_in_ram)))\n",
        "  print('val_labels_in_ram: {}'.format(len(val_labels_in_ram)))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GNm9vyh4s85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if conf['load_in_ram'] == True:\n",
        "  # these two variables will be passed to model.fit_generator\n",
        "  train_it = augmentator_train.flow(train_imgs_in_ram, train_labels_in_ram, conf['batch_size'])\n",
        "  val_it = augmentator_valid.flow(val_imgs_in_ram, val_labels_in_ram, conf['batch_size'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRBQ40GwTj9J",
        "colab_type": "text"
      },
      "source": [
        "**Sanity-check: show an image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtgRKBxAuLFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(train_imgs_in_ram[1]/255.)\n",
        "plt.show()\n",
        "print(train_labels_in_ram[248:252])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1tobWXxXfq8",
        "colab_type": "text"
      },
      "source": [
        "**Function to init the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZpmhWW5XlJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model architecture(34, 50, etc..) and the loss and the\n",
        "# optimizer for training.\n",
        "def init_model():\n",
        "  nb_classes = 3\n",
        "  img_rows, img_cols = conf['image_shape'][1], conf['image_shape'][0]\n",
        "  img_channels = 3\n",
        "\n",
        "  # This will return a Keras Model \n",
        "  model = ResnetBuilder.build_resnet_34((img_channels, img_rows, img_cols), nb_classes)\n",
        "\n",
        "  name_optimizer, params = conf['optimizer']\n",
        "  if name_optimizer == 'sgd':\n",
        "    optimizer = optimizers.SGD(lr=conf['optimizer'][1]['lr'], decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  elif name_optimizer == 'adam':\n",
        "    optimizer = optimizers.Adam(learning_rate=conf['optimizer'][1]['lr'], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  else:\n",
        "    optimizer = optimizers.Adadelta(learning_rate=conf['optimizer'][1]['lr'])\n",
        "\n",
        "  # Configures the model for training\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozySNr8cW3Hm",
        "colab_type": "text"
      },
      "source": [
        "**Check the model: trying to overfit a minibatch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cKtsOanW8ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remember that when trying to overfit a minibatch, \n",
        "#  regularization should be turned off\n",
        "print('Overfitting a minibatch...')\n",
        "\n",
        "model = init_model()\n",
        "\n",
        "# get a minibatch from train dataset\n",
        "batch_func = batch_generator_train(train_list, conf['batch_size'])   # it returns a generator\n",
        "X, y = next(batch_func)  # return a batch\n",
        "\n",
        "class_weight = {0: 1.1,\n",
        "                1: 0.5,\n",
        "                2: 0.7}\n",
        "\n",
        "history = model.fit(x=X, y=y, epochs=7, batch_size=32,\n",
        "                    verbose=1, class_weight=class_weight)\n",
        "\n",
        "# print stats\n",
        "show_report(model, X, y)\n",
        "show_probability_predictions(model, X, y)\n",
        "show_graphs(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7AoscEUhSys",
        "colab_type": "text"
      },
      "source": [
        "**Training our model from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGeCD2tjhX8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if conf['transfer_learning'] == False:\n",
        "  print('Create and compile model...')\n",
        "\n",
        "  model = init_model()\n",
        "\n",
        "  # evaluate also the use of ReduceLROnPlateau before making EarlyStopping, for \n",
        "  # example give to ReduceLROnPlateau a patience of 3 and to EarlyStopping a \n",
        "  # patience of 6.\n",
        "  callbacks = [\n",
        "      EarlyStopping(monitor='val_loss', patience=conf['earlystopping_patience'], verbose=1),\n",
        "      ModelCheckpoint('cervical_best.hdf5', monitor='val_loss', save_best_only=True, verbose=1),\n",
        "  ]\n",
        "\n",
        "  class_weight = {0: 1.1,\n",
        "                  1: 0.5,\n",
        "                  2: 0.7}\n",
        "\n",
        "  print('Fit model...')\n",
        "  # Trains the model on data generated batch by batch \n",
        "  history = model.fit_generator(generator=train_it,\n",
        "                                epochs=conf['nb_epoch'],\n",
        "                                steps_per_epoch=np.floor(len(train_list)/conf['batch_size']),      # the number of batches to consider in an epoch\n",
        "                                validation_data=val_it,\n",
        "                                validation_steps=np.floor(len(valid_list)/conf['batch_size']),\n",
        "                                verbose=1,\n",
        "                                callbacks=callbacks,\n",
        "                                class_weight=class_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rnoN2rRaM1G",
        "colab_type": "text"
      },
      "source": [
        "**Training model with transfer learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hZZtMeRcVU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if conf['transfer_learning'] == True:\n",
        "  print('Create and compile model...')\n",
        "  base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "  # add a fully-connected layer\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "\n",
        "  # add a dropout layer for regularization \n",
        "  dropout = Dropout(0.3)(x)\n",
        "\n",
        "  # and a logistic layer\n",
        "  predictions = Dense(3, activation='softmax')(dropout)\n",
        "\n",
        "  model = Model(input=base_model.input, output=predictions)\n",
        "\n",
        "  # freeze all convolutional Resnet layers\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  model.compile(optimizer=optimizers.SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-o_kFh1bO_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " if conf['transfer_learning'] == True: \n",
        "  callbacks = [\n",
        "      EarlyStopping(monitor='val_loss', patience=conf['earlystopping_patience'], verbose=1),\n",
        "      ModelCheckpoint('cervical_best.hdf5', monitor='val_loss', save_best_only=True, verbose=1),\n",
        "  ]\n",
        "\n",
        "  class_weight = {0: 1.1,\n",
        "                  1: 0.5,\n",
        "                  2: 0.7}\n",
        "\n",
        "  print('Fit model...')\n",
        "  # Trains the model on data generated batch by batch \n",
        "  history = model.fit_generator(generator=train_it,\n",
        "                                epochs=conf['nb_epoch'],\n",
        "                                steps_per_epoch=np.floor(len(train_list)/conf['batch_size']),      # the number of batches to consider in an epoch\n",
        "                                validation_data=val_it,\n",
        "                                validation_steps=np.floor(len(valid_list)/conf['batch_size']),\n",
        "                                verbose=1,\n",
        "                                callbacks=callbacks,\n",
        "                                class_weight=class_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcNkP3QVby43",
        "colab_type": "text"
      },
      "source": [
        "**Print stats of training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E52W8_a9vDwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Stats of training')\n",
        "show_graphs(history, plot_validation=True)\n",
        "\n",
        "print('\\n\\nPrediction on validation dataset and show stats\\n')\n",
        "batch_func = batch_generator_train(valid_list, len(valid_list))   # it returns a generator\n",
        "X, y = next(batch_func)  # return a batch\n",
        "\n",
        "show_report(model, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUXzQy0QJDtb",
        "colab_type": "text"
      },
      "source": [
        "**Save the best state model in local machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i70iBvx5x0v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('cervical_best.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOO_cNcCcB5z",
        "colab_type": "text"
      },
      "source": [
        "**Load the best model found during training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2d6Mt00cCbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('/content/drive/My Drive/cervical_best.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeMprCXacmvU",
        "colab_type": "text"
      },
      "source": [
        "**Prepare for the test phase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXLNM3Qnew1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files  \n",
        "from google.colab import drive\n",
        "import zipfile  \n",
        "\n",
        "!pip install kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "\n",
        "!rm kaggle.json\n",
        "# Upload kaggle API key file\n",
        "uploaded = files.upload()     # upload kaggle.json\n",
        "\n",
        "!rm -rf ../root/.kaggle\n",
        "!mkdir ../root/.kaggle\n",
        "!cp kaggle.json ../root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Download zip file containing the dataset \n",
        "!kaggle competitions download -c intel-mobileodt-cervical-cancer-screening -p /content/drive/My\\ Drive/kaggle_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGQjaKVSmiCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "archive = zipfile.ZipFile('/content/drive/My Drive/kaggle_dataset/intel-mobileodt-cervical-cancer-screening.zip')\n",
        "\n",
        "# Extract zip, for now we will work only on train and \n",
        "# test images folders\n",
        "for file in archive.namelist():\n",
        "    if file.startswith('test_stg2') :\n",
        "        # extract the image with name == file(for example file == train/train/Type_3/465.jpg) \n",
        "        # in the /content/ folderm\n",
        "        archive.extract(file, '/content/') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GEre-G1twwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check number of test images \n",
        "!ls /content/drive/My\\ Drive/test/test | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD4opXh3rCKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "subprocess.call([\"7z\", \"x\", \"-pbyecervicalcancer\", \"/content/test_stg2.7z\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5e4w4uExFsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check number of test images, here they should be 3506\n",
        "!ls /content/drive/My\\ Drive/test_stg2 | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH1OGjWC3ACb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "archive = zipfile.ZipFile('/content/drive/My Drive/kaggle_dataset/intel-mobileodt-cervical-cancer-screening.zip')\n",
        "\n",
        "# Extract zip, for now we will work only on train and \n",
        "# test images folders\n",
        "for file in archive.namelist():\n",
        "  if file.startswith('test'):\n",
        "      # extract the image with name == file(for example file == train/train/Type_3/465.jpg) \n",
        "      # in the /content/ folderm\n",
        "      archive.extract(file, '/content/test_stg2/') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdGogHs5dNbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check number of test images, here they should be 4018\n",
        "!ls /content/test_stg2 | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EClg5gqShcNF",
        "colab_type": "text"
      },
      "source": [
        "**Create submission files with prediction for submission**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYrHwVLzvUna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c intel-mobileodt-cervical-cancer-screening -f sample_submission_stg2.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmKXjMZYJWrU",
        "colab_type": "text"
      },
      "source": [
        "**Load the best training model and make prediction on test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M9O13driNZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('cervical_best.hdf5')    \n",
        "\n",
        "sample_subm = pd.read_csv(\"/content/sample_submission_stg2.csv\")\n",
        "ids = sample_subm['image_name'].values\n",
        "\n",
        "for id in ids:\n",
        "    print('Predict for image {}'.format(id))\n",
        "    files = glob.glob(\"/content/test_stg2/\" + id)\n",
        "    files += glob.glob(\"/content/drive/My Drive/test/test/\" + id)\n",
        "\n",
        "\n",
        "    image_list = []\n",
        "    for f in files:\n",
        "        image = cv2.imread(f)\n",
        "        image = cv2.resize(image, conf['image_shape'])\n",
        "        image_list.append(image)\n",
        "        \n",
        "    image_list = np.array(image_list)\n",
        "\n",
        "    predictions = model.predict(image_list, verbose=1, batch_size=1)\n",
        "\n",
        "    sample_subm.loc[sample_subm['image_name'] == id, 'Type_1'] = predictions[0,0]\n",
        "    sample_subm.loc[sample_subm['image_name'] == id, 'Type_2'] = predictions[0,1]\n",
        "    sample_subm.loc[sample_subm['image_name'] == id, 'Type_3'] = predictions[0,2]\n",
        "    \n",
        "sample_subm.to_csv(\"subm.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UUAO9KQw2tE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat subm.csv"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}