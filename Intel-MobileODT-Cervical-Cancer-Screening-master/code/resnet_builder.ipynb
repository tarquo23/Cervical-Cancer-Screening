{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet_builder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY_Jz7uIZyHm",
        "colab_type": "text"
      },
      "source": [
        "**Helper functions to construct the network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYHLsT1ZZQn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_q3J9mNZUn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    nb_filter = conv_params[\"nb_filter\"]\n",
        "    nb_row = conv_params[\"nb_row\"]\n",
        "    nb_col = conv_params[\"nb_col\"]\n",
        "    subsample = conv_params.setdefault(\"subsample\", (1, 1))\n",
        "    init = conv_params.setdefault(\"init\", \"he_normal\")\n",
        "    border_mode = conv_params.setdefault(\"border_mode\", \"same\")\n",
        "    W_regularizer = conv_params.setdefault(\"W_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        #conv = Convolution2D(nb_filter=nb_filter, nb_row=nb_row, nb_col=nb_col, subsample=subsample,\n",
        "        #                     init=init, border_mode=border_mode, W_regularizer=W_regularizer)(input)\n",
        "        conv = Conv2D(filters=nb_filter, kernel_size=(nb_col,nb_row), strides=subsample,\n",
        "                      kernel_initializer=init, padding=border_mode, kernel_regularizer=W_regularizer)(input)\n",
        "        \n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fhy2j_2ZUw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    # Furthermore, see also https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035\n",
        "    # to justify the order of these three elements BN->relu->conv block  (it is \n",
        "    # called full-preactivation)\n",
        "    nb_filter = conv_params[\"nb_filter\"]\n",
        "    nb_row = conv_params[\"nb_row\"]\n",
        "    nb_col = conv_params[\"nb_col\"]\n",
        "    subsample = conv_params.setdefault(\"subsample\", (1,1))\n",
        "    init = conv_params.setdefault(\"init\", \"he_normal\")\n",
        "    border_mode = conv_params.setdefault(\"border_mode\", \"same\")\n",
        "    W_regularizer = conv_params.setdefault(\"W_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        #return Convolution2D(nb_filter=nb_filter, nb_row=nb_row, nb_col=nb_col, subsample=subsample,\n",
        "        #                     init=init, border_mode=border_mode, W_regularizer=W_regularizer)(activation)\n",
        "        return Conv2D(filters=nb_filter, kernel_size=(nb_col,nb_row), strides=subsample,\n",
        "                      kernel_initializer=init, padding=border_mode, kernel_regularizer=W_regularizer)(activation)\n",
        "\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp7OFDMlZU-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input  # identity\n",
        "\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        #shortcut = Convolution2D(nb_filter=residual_shape[CHANNEL_AXIS],\n",
        "        #                         nb_row=1, nb_col=1,\n",
        "        #                         subsample=(stride_width, stride_height),\n",
        "        #                         init=\"he_normal\", border_mode=\"valid\",\n",
        "        #                         W_regularizer=l2(0.0001))(input)\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1,1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          kernel_initializer=\"he_normal\", padding=\"valid\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "        \n",
        "\n",
        "    #return merge([shortcut, residual], mode=\"sum\")\n",
        "    return concatenate([shortcut, residual])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNBAANl5ZVK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _residual_block(block_function, nb_filter, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_subsample = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_subsample = (2, 2)\n",
        "            # remeber that block_function is a function pointer that will\n",
        "            # point to basic_block or bottleneck.\n",
        "            # each call to this function returns a \n",
        "            input = block_function(nb_filter=nb_filter, init_subsample=init_subsample,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCS_kZGZZVUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_block(nb_filter, init_subsample=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/ 1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    # the scheme that this function creates is the following: \n",
        "    # input -> layer(bn-relu-conv) -> layer(bn-relu-conv) -> output\n",
        "    # with a shortcut from input to output \n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=nb_filter,\n",
        "                           kernel_size=(3,3),\n",
        "                           strides=init_subsample,\n",
        "                           kernel_initializer=\"he_normal\", padding=\"same\",\n",
        "                           kernel_regularizer=l2(0.0001))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(nb_filter=nb_filter, nb_row=3, nb_col=3,\n",
        "                                  subsample=init_subsample)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(nb_filter=nb_filter, nb_row=3, nb_col=3)(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LydNdBPIZVdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bottleneck(nb_filter, init_subsample=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "\n",
        "    Returns:\n",
        "        A final conv layer of nb_filter * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            # conv_1_1 = Conv2D(nb_filter=nb_filter,\n",
        "            #                     nb_row=1, nb_col=1,\n",
        "            #                     subsample=init_subsample,\n",
        "            #                     init=\"he_normal\", border_mode=\"same\",\n",
        "            #                     W_regularizer=l2(0.0001))(input)\n",
        "            conv_1_1 = Conv2D(filters=nb_filter,\n",
        "                              kernel_size=(1,1),\n",
        "                              strides=init_subsample,\n",
        "                              kernel_initializer=\"he_normal\", padding=\"same\",\n",
        "                              kernel_regularizer=l2(0.0001))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(nb_filter=nb_filter, nb_row=1, nb_col=1,\n",
        "                                     subsample=init_subsample)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(nb_filter=nb_filter, nb_row=3, nb_col=3)(conv_1_1)\n",
        "        residual = _bn_relu_conv(nb_filter=nb_filter * 4, nb_row=1, nb_col=1)(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFuGjbQMZor5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.common.image_dim_ordering() == 'tf':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        CHANNEL_AXIS = 3\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIfshfHhZpLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXS2SupQZ79_",
        "colab_type": "text"
      },
      "source": [
        "**Functions to get a ResNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVkP13UXZpVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
        "        # in case of: build_resnet_34: input_shape == (img_channels, img_rows, img_cols)\n",
        "        #                              repetitions == [3, 4, 6, 3]\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary\n",
        "        if K.common.image_dim_ordering() == 'tf':\n",
        "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        # block_fn is a function and its value will be basicblock\n",
        "        # or bottleneck depending on the network. \n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        # This is the first layer.\n",
        "        input = Input(shape=input_shape)\n",
        "        # _conv_bn_relu is used to build a [conv->bn->relu] block\n",
        "        conv1 = _conv_bn_relu(nb_filter=64, nb_row=7, nb_col=7, subsample=(2, 2))(input)    # 64 filters 7x7 with /2 downsampling\n",
        "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "        block = pool1\n",
        "\n",
        "        nb_filter = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            # Pass here(in _residual_block) the block_fn function because depending on\n",
        "            # the resnet architecture the residual block will be bottleneck \n",
        "            # or basic block.\n",
        "            # This function creates a number of residual blocks equal to repetitions.\n",
        "            # _residual_block: returns a function pointer that we execute by \n",
        "            # giving it the block as input.\n",
        "            # nb_filter: the number of filters at each layer\n",
        "            block = _residual_block(block_fn, nb_filter=nb_filter, repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            \n",
        "            # At the end of each \"macro-block\" duplicate the number of filter, \n",
        "            # this will be the number of filters in the next macro-block.\n",
        "            # e.g. number of filters at each macro-block of a \n",
        "            # resnet_34:  64 -> 128 -> 256 -> 512 \n",
        "            nb_filter *= 2\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        #block_norm = BatchNormalization(mode=0, axis=CHANNEL_AXIS)(block)\n",
        "        block_norm = BatchNormalization(axis=CHANNEL_AXIS)(block)\n",
        "        block_output = Activation(\"relu\")(block_norm)\n",
        "\n",
        "        # Classifier block\n",
        "        block_shape = K.int_shape(block)\n",
        "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
        "                                 strides=(1, 1))(block_output)\n",
        "        flatten1 = Flatten()(pool2)\n",
        "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\", activation=\"softmax\")(flatten1)\n",
        "        #dense = Dense(output_dim=num_outputs, W_regularizer=l2(0.01), init=\"he_normal\", activation=\"linear\")(flatten1)\n",
        "\n",
        "        # Keras Model: now we create the instance of the model given \n",
        "        # an input tensor and an output tensor.\n",
        "        # This model will include all layers needed to compute dense given input.\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_test(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [1, 1, 1, 1])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs):\n",
        "        # input_shape == (img_channels, img_rows, img_cols)\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}